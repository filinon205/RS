{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.metrics import precision_at_k, recall_at_k\n",
    "from src.metrics import precision_at_k\n",
    "from src.utils import prefilter_items\n",
    "from src.recommenders import MainRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disabling warnings entirely\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightfm in c:\\users\\nikita.saprykin\\anaconda3\\lib\\site-packages (1.16)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nikita.saprykin\\anaconda3\\lib\\site-packages (from lightfm) (0.23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\nikita.saprykin\\anaconda3\\lib\\site-packages (from lightfm) (2.25.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\nikita.saprykin\\anaconda3\\lib\\site-packages (from lightfm) (1.5.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\nikita.saprykin\\anaconda3\\lib\\site-packages (from lightfm) (1.19.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\nikita.saprykin\\anaconda3\\lib\\site-packages (from requests->lightfm) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\nikita.saprykin\\anaconda3\\lib\\site-packages (from requests->lightfm) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikita.saprykin\\anaconda3\\lib\\site-packages (from requests->lightfm) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nikita.saprykin\\anaconda3\\lib\\site-packages (from requests->lightfm) (1.26.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\nikita.saprykin\\anaconda3\\lib\\site-packages (from scikit-learn->lightfm) (1.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nikita.saprykin\\anaconda3\\lib\\site-packages (from scikit-learn->lightfm) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "!pip install lightfm\n",
    "# Для работы с матрицами\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "\n",
    "# Матричная факторизация\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import bm25_weight, tfidf_weight\n",
    "import  implicit\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k, recall_at_k\n",
    "\n",
    "# Функции из 1-ого вебинара\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\nikita.saprykin\\Desktop\\RCS\\урок_5\\retail_train.csv')\n",
    "\n",
    "data = data.sample(2000, random_state = 2021) # ограничение по записям 2000\n",
    "\n",
    "\n",
    "item_features = pd.read_csv(r'C:\\Users\\nikita.saprykin\\Desktop\\RCS\\урок_5\\product.csv')\n",
    "user_features = pd.read_csv(r'C:\\Users\\nikita.saprykin\\Desktop\\RCS\\урок_5\\hh_demographic.csv')\n",
    "\n",
    "# column processing\n",
    "item_features.columns = [col.lower() for col in item_features.columns]\n",
    "user_features.columns = [col.lower() for col in user_features.columns]\n",
    "\n",
    "item_features.rename(columns={'product_id': 'item_id'}, inplace=True)\n",
    "user_features.rename(columns={'household_key': 'user_id'}, inplace=True)\n",
    "\n",
    "# train test split\n",
    "test_size_weeks = 3\n",
    "\n",
    "data_train = data[data['week_no'] < data['week_no'].max() - test_size_weeks]\n",
    "data_test = data[data['week_no'] >= data['week_no'].max() - test_size_weeks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Filter items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items_before = data_train['item_id'].nunique()\n",
    "\n",
    "#TO SPPED IT UP (INCREASE in PRODUCT)\n",
    "#TO SPPED IT UP (INCREASE in PRODUCT)\n",
    "#TO SPPED IT UP (INCREASE in PRODUCT)\n",
    "#TO SPPED IT UP (INCREASE in PRODUCT)\n",
    "#TO SPPED IT UP (INCREASE in PRODUCT)\n",
    "take_n_popular = 1000\n",
    "#TO SPPED IT UP (INCREASE in PRODUCT)\n",
    "#TO SPPED IT UP (INCREASE in PRODUCT)\n",
    "#TO SPPED IT UP (INCREASE in PRODUCT)\n",
    "#TO SPPED IT UP (INCREASE in PRODUCT)\n",
    "#TO SPPED IT UP (INCREASE in PRODUCT)\n",
    "#TO SPPED IT UP (INCREASE in PRODUCT)\n",
    "\n",
    "\n",
    "data_train_filtered = prefilter_items(data_train, take_n_popular=take_n_popular, item_features=item_features)\n",
    "\n",
    "n_items_after = data_train_filtered['item_id'].nunique()\n",
    "print('Decreased # items from {} to {}'.format(n_items_before, n_items_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Prepare csr TRAIN matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_item_matrix = pd.pivot_table(data_train, \n",
    "                                  index='user_id', columns='item_id', \n",
    "                                  values='quantity', # Можно пробоват ьдругие варианты\n",
    "                                  aggfunc='count', \n",
    "                                  fill_value=0\n",
    "                                 )\n",
    "\n",
    "train_user_item_matrix = train_user_item_matrix.astype(float) # необходимый тип матрицы для implicit\n",
    "\n",
    "# переведем в формат sparse matrix\n",
    "sparse_train_user_item = csr_matrix(train_user_item_matrix).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Prepare csr TEST matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test[data_test['item_id'].isin(data_train['item_id'].unique())]\n",
    "\n",
    "test_user_item_matrix = pd.pivot_table(data_test, \n",
    "                                  index='user_id', columns='item_id', \n",
    "                                  values='quantity', # Можно пробоват ьдругие варианты\n",
    "                                  aggfunc='count', \n",
    "                                  fill_value=0\n",
    "                                 )\n",
    "\n",
    "test_user_item_matrix = test_user_item_matrix.astype(float) # необходимый тип матрицы для implicit\n",
    "\n",
    "sparse_test_user_item = csr_matrix(test_user_item_matrix).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_itemid, id_to_userid, itemid_to_id, userid_to_id =\\\n",
    "MainRecommender.prepare_dicts(train_user_item_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare user and item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feat = pd.DataFrame(train_user_item_matrix.index)\n",
    "user_feat = user_feat.merge(user_features, on='user_id', how='left')\n",
    "user_feat.set_index('user_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feat = pd.DataFrame(train_user_item_matrix.columns)\n",
    "item_feat = item_feat.merge(item_features, on='item_id', how='left')\n",
    "item_feat.set_index('item_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feat_lightfm = pd.get_dummies(user_feat, columns=user_feat.columns.tolist())\n",
    "item_feat_lightfm = pd.get_dummies(item_feat, columns=item_feat.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing input data for the LifgtFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features=csr_matrix(user_feat_lightfm.values).tocsr()\n",
    "item_features=csr_matrix(item_feat_lightfm.values).tocsr()\n",
    "\n",
    "sample_weight=coo_matrix(train_user_item_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightFM(no_components=40,\n",
    "#                 loss='bpr',\n",
    "                loss='warp',\n",
    "                learning_rate=0.05, \n",
    "                item_alpha=0.1,\n",
    "                user_alpha=0.1, \n",
    "                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x27b72a773a0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit((sparse_train_user_item > 0) * 1,  # user-item matrix из 0 и 1\n",
    "          sample_weight=sample_weight,\n",
    "          user_features=user_features,\n",
    "          item_features=item_features,\n",
    "          epochs=15, \n",
    "          num_threads=4,\n",
    "          verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_emb = model.get_user_representations(features=csr_matrix(user_feat_lightfm.values).tocsr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_emb = model.get_item_representations(features=csr_matrix(item_feat_lightfm.values).tocsr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011275415"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_precision = precision_at_k(model,\n",
    "                                 sparse_train_user_item, \n",
    "                                 user_features=csr_matrix(user_feat_lightfm.values).tocsr(),\n",
    "                                 item_features=csr_matrix(item_feat_lightfm.values).tocsr(),\n",
    "                                 k=5).mean()\n",
    "\n",
    "train_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.0927978, -3.0772839, -2.9638767, -2.770964 , -2.99987  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_item_ids = np.array([1, 2, 3, 200, 1200])\n",
    "\n",
    "predictions = model.predict(user_ids=0,\n",
    "                            item_ids=test_item_ids,\n",
    "                            user_features=csr_matrix(user_feat_lightfm.values).tocsr(),\n",
    "                            item_features=csr_matrix(item_feat_lightfm.values).tocsr(),\n",
    "                            num_threads=4)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03529412"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_precision = precision_at_k(model, \n",
    "                                sparse_test_user_item, \n",
    "                                user_features=csr_matrix(user_feat_lightfm.values).tocsr(),\n",
    "                                item_features=csr_matrix(item_feat_lightfm.values).tocsr(),\n",
    "                                k=5).mean()\n",
    "\n",
    "test_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Прочитать статьи про BPR, WARP loss, learning-to-rank*\n",
    "\n",
    "2) Сделать грид серч текущей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def dict_configs(d):\n",
    "    for vcomb in product(*d.values()):\n",
    "        yield dict(zip(d.keys(), vcomb))\n",
    "\n",
    "param_grid = {\n",
    "            \"no_components\": np.arange(25,55,25),\n",
    "#             \"learning_schedule\": [\"adagrad\"], #\"adadelta\"\n",
    "            \"loss\": [\"warp\"], #\"bpr\"\n",
    "            \"learning_rate\": np.arange(.01,.1,.05),\n",
    "            \"item_alpha\": np.arange (.01,.1,.05),\n",
    "            \"user_alpha\": np.arange (.01,.1,.05),\n",
    "#             \"max_sampled\": np.arange(1,101,25),\n",
    "            \"num_epochs\": np.arange(10,70,30),\n",
    "            \"random_state\":[42],\n",
    "        }\n",
    "\n",
    "def grid_search(sparce_matrix_train, \n",
    "                  test, \n",
    "                  sample_weight, \n",
    "                  user_features,\n",
    "                  item_features,\n",
    "                num_threads=8):\n",
    "\n",
    "    for hyperparams in dict_configs(param_grid):\n",
    "        num_epochs = hyperparams.pop(\"num_epochs\")\n",
    "       \n",
    "        model = LightFM(**hyperparams)\n",
    "        \n",
    "        model.fit(sparse_train_user_item,\n",
    "                  sample_weight=sample_weight,\n",
    "                  user_features=user_features,\n",
    "                  item_features=item_features,\n",
    "                  epochs=num_epochs, \n",
    "                  num_threads=num_threads,\n",
    "                  verbose=False)\n",
    "\n",
    "        score = precision_at_k(model,\n",
    "                               test,\n",
    "                               user_features=user_features,\n",
    "                               item_features=item_features,\n",
    "                               k=5).mean()\n",
    "\n",
    "        hyperparams[\"num_epochs\"] = num_epochs\n",
    "\n",
    "        yield (score, hyperparams, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 0.052941177040338516 at {'no_components': 50, 'loss': 'warp', 'learning_rate': 0.060000000000000005, 'item_alpha': 0.01, 'user_alpha': 0.01, 'random_state': 42, 'num_epochs': 40}\n"
     ]
    }
   ],
   "source": [
    "from lightfm.datasets import fetch_movielens\n",
    "(score, hyperparams, model) = max(grid_search(sparse_train_user_item, \n",
    "                                                sparse_test_user_item,\n",
    "                                                sample_weight=sample_weight,\n",
    "                                                user_features=user_features,\n",
    "                                                item_features=item_features,\n",
    "                                             ),\n",
    "                                  key=lambda x: x[0])\n",
    "\n",
    "print(\"Best score {} at {}\".format(score, hyperparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
